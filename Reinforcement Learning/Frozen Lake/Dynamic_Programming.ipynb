{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project: Dynamic Programming\n",
    "\n",
    "In this notebook, you will write your own implementations of many classical dynamic programming algorithms.  \n",
    "\n",
    "While we have provided some starter code, you are welcome to erase these hints and write your code from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Explore FrozenLakeEnv\n",
    "\n",
    "Use the code cell below to create an instance of the [FrozenLake](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frozenlake import FrozenLakeEnv\n",
    "\n",
    "env = FrozenLakeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent moves through a $4 \\times 4$ gridworld, with states numbered as follows:\n",
    "```\n",
    "[[ 0  1  2  3]\n",
    " [ 4  5  6  7]\n",
    " [ 8  9 10 11]\n",
    " [12 13 14 15]]\n",
    "```\n",
    "and the agent has 4 potential actions:\n",
    "```\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "```\n",
    "\n",
    "Thus, $\\mathcal{S}^+ = \\{0, 1, \\ldots, 15\\}$, and $\\mathcal{A} = \\{0, 1, 2, 3\\}$.  Verify this by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n",
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# print the state space and action space\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "# print the total number of states and actions\n",
    "print(env.nS)\n",
    "print(env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic programming assumes that the agent has full knowledge of the MDP.  We have already amended the `frozenlake.py` file to make the one-step dynamics accessible to the agent.  \n",
    "\n",
    "Execute the code cell below to return the one-step dynamics corresponding to a particular state and action.  In particular, `env.P[1][0]` returns the the probability of each possible reward and next state, if the agent is in state 1 of the gridworld and decides to go left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry takes the form \n",
    "```\n",
    "prob, next_state, reward, done\n",
    "```\n",
    "where: \n",
    "- `prob` details the conditional probability of the corresponding (`next_state`, `reward`) pair, and\n",
    "- `done` is `True` if the `next_state` is a terminal state, and otherwise `False`.\n",
    "\n",
    "Thus, we can interpret `env.P[1][0]` as follows:\n",
    "$$\n",
    "\\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=1,A_t=0) = \\begin{cases}\n",
    "               \\frac{1}{3} \\text{ if } s'=1, r=0\\\\\n",
    "               \\frac{1}{3} \\text{ if } s'=0, r=0\\\\\n",
    "               \\frac{1}{3} \\text{ if } s'=5, r=0\\\\\n",
    "               0 \\text{ else}\n",
    "            \\end{cases}\n",
    "$$\n",
    "\n",
    "Feel free to change the code cell above to explore how the environment behaves in response to other (state, action) pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Iterative Policy Evaluation\n",
    "\n",
    "In this section, you will write your own implementation of iterative policy evaluation.\n",
    "\n",
    "Your algorithm should accept four arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `theta`: This is a very small positive number that is used to decide if the estimate has sufficiently converged to the true value function (default value: `1e-8`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s` under the input policy.\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 7, 0, True)],\n",
       " 1: [(1.0, 7, 0, True)],\n",
       " 2: [(1.0, 7, 0, True)],\n",
       " 3: [(1.0, 7, 0, True)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bellman_equation_state_value(state, policy, env, gamma, V):\n",
    "    state_value = 0\n",
    "    for action, policy_a_s in enumerate(policy[state]):\n",
    "        for prob, next_state, reward, done in env.P[state][action]:\n",
    "            state_value += policy_a_s*prob*(reward +(gamma*V[next_state]))\n",
    "    return state_value\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS, dtype=np.float32)\n",
    "    delta = np.ones(env.nS, dtype=np.float32)\n",
    "    while np.any(delta>theta):\n",
    "        delta = np.zeros(env.nS, dtype=np.float32)\n",
    "        for s in range(V.shape[0]):\n",
    "            v = V[s]\n",
    "            V[s] = bellman_equation_state_value(s, policy, env, gamma, V)\n",
    "            delta[s] = max(delta[s], abs(v-V[s]))\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the equiprobable random policy $\\pi$, where $\\pi(a|s) = \\frac{1}{|\\mathcal{A}(s)|}$ for all $s\\in\\mathcal{S}$ and $a\\in\\mathcal{A}(s)$.  \n",
    "\n",
    "Use the code cell below to specify this policy in the variable `random_policy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to evaluate the equiprobable random policy and visualize the output.  The state-value function has been reshaped to match the shape of the gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8fb5aa0fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plot_utils import plot_values\n",
    "\n",
    "# evaluate the policy \n",
    "V = policy_evaluation(env, random_policy)\n",
    "\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to test your function.  If the code cell returns **PASSED**, then you have implemented the function correctly!  \n",
    "\n",
    "**Note:** In order to ensure accurate results, make sure that your `policy_evaluation` function satisfies the requirements outlined above (with four inputs, a single output, and with the default values of the input arguments unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**<span style=\"color: green;\">PASSED</span>**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import check_test\n",
    "\n",
    "check_test.run_check('policy_evaluation_check', policy_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Obtain $q_\\pi$ from $v_\\pi$\n",
    "\n",
    "In this section, you will write a function that takes the state-value function estimate as input, along with some state $s\\in\\mathcal{S}$.  It returns the **row in the action-value function** corresponding to the input state $s\\in\\mathcal{S}$.  That is, your function should accept as input both $v_\\pi$ and $s$, and return $q_\\pi(s,a)$ for all $a\\in\\mathcal{A}(s)$.\n",
    "\n",
    "Your algorithm should accept four arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "- `s`: This is an integer corresponding to a state in the environment.  It should be a value between `0` and `(env.nS)-1`, inclusive.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `q`: This is a 1D numpy array with `q.shape[0]` equal to the number of actions (`env.nA`).  `q[a]` contains the (estimated) value of state `s` and action `a`.\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_from_v(env, V, s, gamma=1):\n",
    "    q = np.zeros(env.nA)\n",
    "    for a in range(env.nA):\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            q[a] += prob*(reward + gamma*V[next_state]) \n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to print the action-value function corresponding to the above state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-Value Function:\n",
      "[[ 0.0147094   0.01393978  0.01393978  0.01317015]\n",
      " [ 0.00852356  0.01163091  0.01086129  0.01550788]\n",
      " [ 0.02444514  0.02095297  0.02406033  0.01435346]\n",
      " [ 0.01047649  0.01047649  0.00698432  0.01396865]\n",
      " [ 0.02166487  0.01701828  0.01624865  0.01006281]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.05433537  0.04735105  0.05433537  0.00698432]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.01701828  0.04099204  0.03480619  0.04640825]\n",
      " [ 0.07020885  0.11755989  0.10595783  0.05895311]\n",
      " [ 0.1894042   0.17582036  0.16001423  0.04297382]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.08799676  0.20503717  0.23442714  0.17582036]\n",
      " [ 0.25238821  0.5383705   0.52711476  0.43929116]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros([env.nS, env.nA])\n",
    "for s in range(env.nS):\n",
    "    Q[s] = q_from_v(env, V, s)\n",
    "print(\"Action-Value Function:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to test your function.  If the code cell returns **PASSED**, then you have implemented the function correctly!  \n",
    "\n",
    "**Note:** In order to ensure accurate results, make sure that the `q_from_v` function satisfies the requirements outlined above (with four inputs, a single output, and with the default values of the input arguments unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**<span style=\"color: green;\">PASSED</span>**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_test.run_check('q_from_v_check', q_from_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Policy Improvement\n",
    "\n",
    "In this section, you will write your own implementation of policy improvement. \n",
    "\n",
    "Your algorithm should accept three arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "\n",
    "Please complete the function in the code cell below.  You are encouraged to use the `q_from_v` function you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=1):\n",
    "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    for s in range(env.nS):\n",
    "        q = q_from_v(env, V, s, gamma=1)\n",
    "        # Checks in which index you have the 1s, and returns the array with index\n",
    "        index = np.argwhere(q==np.max(q)).flatten()\n",
    "        policy[s] = 0\n",
    "        for i in index:\n",
    "            policy[s,i] = 1/len(index)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to test your function.  If the code cell returns **PASSED**, then you have implemented the function correctly!  \n",
    "\n",
    "**Note:** In order to ensure accurate results, make sure that the `policy_improvement` function satisfies the requirements outlined above (with three inputs, a single output, and with the default values of the input arguments unchanged).\n",
    "\n",
    "Before moving on to the next part of the notebook, you are strongly encouraged to check out the solution in **Dynamic_Programming_Solution.ipynb**.  There are many correct ways to approach this function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**<span style=\"color: green;\">PASSED</span>**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_test.run_check('policy_improvement_check', policy_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Policy Iteration\n",
    "\n",
    "In this section, you will write your own implementation of policy iteration.  The algorithm returns the optimal policy, along with its corresponding state-value function.\n",
    "\n",
    "Your algorithm should accept three arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `theta`: This is a very small positive number that is used to decide if the policy evaluation step has sufficiently converged to the true value function (default value: `1e-8`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "\n",
    "Please complete the function in the code cell below.  You are strongly encouraged to use the `policy_evaluation` and `policy_improvement` functions you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def policy_iteration(env, gamma=1, theta=1e-8):\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        V = policy_evaluation(env, policy, gamma=gamma, theta=theta)\n",
    "        new_policy = policy_improvement(env, V, gamma=gamma)\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            policy = new_policy\n",
    "            break\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to solve the MDP and visualize the output.  The optimal state-value function has been reshaped to match the shape of the gridworld.\n",
    "\n",
    "**Compare the optimal state-value function to the state-value function from Part 1 of this notebook**.  _Is the optimal state-value function consistently greater than or equal to the state-value function for the equiprobable random policy?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\n",
      "[[ 1.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    1.  ]\n",
      " [ 0.    0.    0.    1.  ]\n",
      " [ 0.    0.    0.    1.  ]\n",
      " [ 1.    0.    0.    0.  ]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.5   0.    0.5   0.  ]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.    0.    0.    1.  ]\n",
      " [ 0.    1.    0.    0.  ]\n",
      " [ 1.    0.    0.    0.  ]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.    0.    1.    0.  ]\n",
      " [ 0.    1.    0.    0.  ]\n",
      " [ 0.25  0.25  0.25  0.25]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8fb1378128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain the optimal policy and optimal state-value function\n",
    "policy_pi, V_pi = policy_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_pi,\"\\n\")\n",
    "\n",
    "plot_values(V_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to test your function.  If the code cell returns **PASSED**, then you have implemented the function correctly!  \n",
    "\n",
    "**Note:** In order to ensure accurate results, make sure that the `policy_iteration` function satisfies the requirements outlined above (with three inputs, two outputs, and with the default values of the input arguments unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**<span style=\"color: green;\">PASSED</span>**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_test.run_check('policy_iteration_check', policy_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Truncated Policy Iteration\n",
    "\n",
    "In this section, you will write your own implementation of truncated policy iteration.  \n",
    "\n",
    "You will begin by implementing truncated policy evaluation.  Your algorithm should accept five arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "- `max_it`: This is a positive integer that corresponds to the number of sweeps through the state space (default value: `1`).\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_equation_state_value(state, policy, env, gamma, V):\n",
    "    state_value = 0\n",
    "    for action, policy_a_s in enumerate(policy[state]):\n",
    "        for prob, next_state, reward, done in env.P[state][action]:\n",
    "            state_value += policy_a_s*prob*(reward +(gamma*V[next_state]))\n",
    "    return state_value\n",
    "\n",
    "def truncated_policy_evaluation(env, policy, V, max_it=1, gamma=1):\n",
    "    V = np.zeros(env.nS, dtype=np.float32)\n",
    "    delta = np.ones(env.nS, dtype=np.float32)\n",
    "    iteration = 0\n",
    "    while iteration<max_it:\n",
    "        for s in range(V.shape[0]):\n",
    "            V[s] = bellman_equation_state_value(s, policy, env, gamma, V)\n",
    "        iteration+=1\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement truncated policy iteration.  Your algorithm should accept five arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `max_it`: This is a positive integer that corresponds to the number of sweeps through the state space (default value: `1`).\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `theta`: This is a very small positive number that is used for the stopping criterion (default value: `1e-8`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_policy_iteration(env, max_it=1, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        V = truncated_policy_evaluation(env, policy, V, max_it=1, gamma=gamma)\n",
    "        new_policy = policy_improvement(env, V, gamma=gamma)\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            policy = new_policy\n",
    "            break\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to solve the MDP and visualize the output.  The state-value function has been reshaped to match the shape of the gridworld.\n",
    "\n",
    "Play with the value of the `max_it` argument.  Do you always end with the optimal state-value function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAFoCAYAAAD5IVjuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAF0JJREFUeJzt3H1wXXWdx/HPl95cQtNCa1tTEOkD4UFXnLa51rbj2HXYTUHEh53RZWErYuuITqd11C0Vhx10fdxhhWUnbabBVewq2bUujgq6pHUbKym73A4FuikPUhoflliKCwgobcN3/zg34d7mNr1pku+9N/f9yvzGyTnfc87vfnv53HPPOcbcXQCA8XVKuScAALWAsAWAAIQtAAQgbAEgAGELAAEIWwAIQNiiopjZATP7s3LPYzyY2VVmdk+554HyIGyrnJm9zcy6zew5M/udmd1rZm/JrfuQmf18BPuaa2ZuZqmTnMtnzOxnRZbPNLPDZvamk9nvWDCzb+bm8ELe+MtxPN6QXrr7t929ZbyOicpG2FYxMztd0o8k/ZOk10h6naTPSXq5TFPaImmZmc07ZvkVkh52971lmFO+v3f3KXnjX8s8H9QQwra6nS9J7n6Hu/e7+x/c/R53f8jM3iCpTdLS3Fncs5JkZpeZ2QNm9ryZ/crMbszb38BZ6bO5bZbmtvmwme0zs/8zs/8wsznFJuPuv5b0U0krj1n1QUm35/Z1rpn91MyeMbNDZvZtM5tWbH+5s9Ev5P3+p2b267zfzzKz75nZ02b2pJmtLblzhcdxM2sqdtyBY5rZp8zsoJk9ZWbX5NWeZmb/YGa9uW8XPzez01Skl8d+0zCzZWZ2f267+81sWd66HWb2d7lvKr83s3vMbObJvD5UBsK2uj0mqd/MbjezS81s+sAKd98n6VpJu3JncQOB9qKS8Jsm6TJJHzOz9+bWvT33v9Ny2+zKrbte0l9ImiVpp6Q7hpnT7coLWzO7QNKCvG1M0pclnSXpDZJeL+nGkb5wMztF0g8lPajkjP5iSZ8wsxUj3VcJZks6I3ecVZJa83p9k6RmScuUfLtYL+kVFenlMfN/jaS7JN0qaYakr0m6y8xm5JVdKekaSa+VlJb06bF/aYhC2FYxd39e0tskuaR2SU+b2Q/MrHGYbXa4+8Pu/oq7P6QkBJcPc5iPSvqyu+9z96OSviRpwfHObiXdKakx7yztg5J+7O5P547/C3fvdPeXc8u+doLjH89bJM1y98+7+2F336+kB1cMs82nzezZ3Dg0gmMdkfR5dz/i7ndLekHSBbnA/7Ckde7+m9y3i253L+UyzmWSHnf3Le5+1N3vkPSIpMvzar7h7o+5+x8k/ZuSDy1UKcK2yuVC8EPufrakNyk5Y7zlePVm9lYz+8/cV+/nlJz9Dvf1dI6kfxwIKUm/U3J2+jozuz7vZlNbbj4vSfqupA+amUm6SrlLCLnjv9bMOszsN2b2vKR/OcHxh5vXWXnh+aySM/DjftBIusndp+XGSI75TO6DZsBLkqYomXe9pCdGOnkl/069xyzrVXL2PKCvyDFRpQjbCcTdH5H0TSWhKyVnvMf6jqQfSHq9u5+h5LquDVP/K0kfzQupae5+Wu4M7kt5N5uuzdvmdkkfkPTnkqYquYk34Mu547zZ3U+X9Nd5xz/Wi5Im5/0++5h5PXnMvKa6+zuPs6/hvDTMcYZzSNIfJZ1bZN2J/pze/yr5wMh3jqTflHhsVBnCtoqZ2YW5Gzdn535/vaS/knRfruS3ks42s3TeZlMl/c7d/2hmi5VcFxzwtJLrjfPzlrVJ+oyZ/UnuGGeY2ftPMLWdkp6VtFlSh7sfPub4Lyi5cfQ6SX8zzH72SHqnmb3GzGZL+kTeuv+W9LyZXZe7STXJzN5kucfeRmiPpCtz+7hEJV7WcPdXJP2zpK/lbtZNyt0IO1XFe5nvbknnm9mVZpay5DG0N6rwgwkTCGFb3X4v6a2S/svMXlQSsnslfSq3/qeS/kdSX941yo9L+ryZ/V7S3yq5Fihp8BLAFyXdm/tqvsTd75T0VUkdua/9eyVdOtykPPkjyd9Scub2rWNWf07SIknPKblB9O/D7GqLkhtgByTdI2nwUS1371dyfXOBpCeVnGXepuRG1kity+3rWSWXPb4/gm0/LelhSfcrucTyVUmnFOtl/kbu/oykdyn5t3pGyY21d7n7SK4lo4oYfzwcAMYfZ7YAEICwBYAAhC0ABCBsASAAYQsAAUb0p/Rs5kzX3LnjNBUAqEIHDsgPHTre/zFn0Mj+buncuVI2e7JTAoCJJ5MpqYzLCAAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCAsAWAAIQtAAQgbAEgAGELAAEIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCgusJ240Zp3jypvl5qbpZ27hy+vqsrqauvl+bPl9raYuZZ6ejj6NHDsVFLfXT3koeam0svHuvR0eFKpVybN7t6elxr1rgaGly9vcXr9+93TZ6c1PX0JNulUq6tW8v3Giph0Ed6WCljovSxudlLKRvRPssatosXu1avLlzW1OTasKF4/fr1yfr8ZatWuZYsKf+brJyDPtLDShkTpY8lhm11XEY4fFjavVtqaSlc3tIidXcX32bXrqH1K1ZI2ax05Mj4zLPS0cfRo4djowb7WB1he+iQ1N8vNTYWLm9slPr6im/T11e8/ujRZH+1iD6OHj0cGzXYx+oI2wFmhb+7D112ovpiy2sNfRw9ejg2aqiP1RG2M2dKkyYN/cQ7eHDoJ92A2bOL16dS0owZ4zPPSkcfR48ejo0a7GN1hG06nTzu0dlZuLyzU1q2rPg2S5dK27YNrc9kpLq68ZlnpaOPo0cPx0Yt9nEkN93K/uhXXZ2rvT157GPt2uQxkQMHkvUrVyZjoH7gMZF165L69vZk+3I/JlLuQR/pYaWMidLHCffol7urtdU1Z44rnXYtWuTq6np13fLlyciv37HDtXBhUj93rmvTpvK/wSph0Ed6WCljIvSxxLA1dy/5LNgyGVc2O47n2QBQZTIZeTZ7wjt01XHNFgCqHGELAAEIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCAsAWAAIQtAAQgbAEgAGELAAEIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCAsAWAAIQtAAQgbAEgAGELAAEIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCAsAWAANUVths3SvPmSfX1UnOztHPn8PVdXUldfb00f77U1hYzz0pHH0ePHo6NWuqju5c81NxcevFYj44OVyrl2rzZ1dPjWrPG1dDg6u0tXr9/v2vy5KSupyfZLpVybd1avtdQCYM+0sNKGROlj83NXkrZiPZZ1rBdvNi1enXhsqYm14YNxevXr0/W5y9btcq1ZEn532TlHPSRHlbKmCh9LDFsq+MywuHD0u7dUktL4fKWFqm7u/g2u3YNrV+xQspmpSNHxmeelY4+jh49HBs12MfqCNtDh6T+fqmxsXB5Y6PU11d8m76+4vVHjyb7q0X0cfTo4diowT5WR9gOMCv83X3oshPVF1tea+jj6NHDsVFDfayOsJ05U5o0aegn3sGDQz/pBsyeXbw+lZJmzBifeVY6+jh69HBs1GAfqyNs0+nkcY/OzsLlnZ3SsmXFt1m6VNq2bWh9JiPV1Y3PPCsdfRw9ejg2arGPI7npVvZHv+rqXO3tyWMfa9cmj4kcOJCsX7kyGQP1A4+JrFuX1Le3J9uX+zGRcg/6SA8rZUyUPk64R7/cXa2trjlzXOm0a9EiV1fXq+uWL09Gfv2OHa6FC5P6uXNdmzaV/w1WCYM+0sNKGROhjyWGrbl7yWfBlsm4stlxPM8GgCqTyciz2RPeoauOa7YAUOUIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCAsAWAAIQtAAQgbAEgAGELAAEIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCAsAWAAIQtAAQgbAEgAGELAAEIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCAsAWAANUVths3SvPmSfX1UnOztHPn8PVdXUldfb00f77U1hYzz0pHH0ePHo6NWuqju5c81NxcevFYj44OVyrl2rzZ1dPjWrPG1dDg6u0tXr9/v2vy5KSupyfZLpVybd1avtdQCYM+0sNKGROlj83NXkrZiPZZ1rBdvNi1enXhsqYm14YNxevXr0/W5y9btcq1ZEn532TlHPSRHlbKmCh9LDFsq+MywuHD0u7dUktL4fKWFqm7u/g2u3YNrV+xQspmpSNHxmeelY4+jh49HBs12MfqCNtDh6T+fqmxsXB5Y6PU11d8m76+4vVHjyb7q0X0cfTo4diowT5WR9gOMCv83X3oshPVF1tea+jj6NHDsVFDfayOsJ05U5o0aegn3sGDQz/pBsyeXbw+lZJmzBifeVY6+jh69HBs1GAfqyNs0+nkcY/OzsLlnZ3SsmXFt1m6VNq2bWh9JiPV1Y3PPCsdfRw9ejg2arGPI7npVvZHv+rqXO3tyWMfa9cmj4kcOJCsX7kyGQP1A4+JrFuX1Le3J9uX+zGRcg/6SA8rZUyUPk64R7/cXa2trjlzXOm0a9EiV1fXq+uWL09Gfv2OHa6FC5P6uXNdmzaV/w1WCYM+0sNKGROhjyWGrbl7yWfBlsm4stlxPM8GgCqTyciz2RPeoauOa7YAUOUIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCAsAWAAIQtAAQgbAEgAGELAAEIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAgFS5JwCcDLdyz2BiMC/3DGoHZ7YAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCAsAWAAIQtAAQgbAEgAGELAAEIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEqK6w3bhRmjdPqq+XmpulnTuHr+/qSurq66X586W2tph5Vroa7+NGbdQ8zVO96tWsZu3U8V9/l7q0TMs0QzN0mk7ThbpQN+mmgprv6rvKKKNpmqYGNWiBFuh23V5Q06pWvVlv1um5n6Vaqrt0V0HNDbpBF+pCNahB0zVdF+tidau7oOYj+ojO1bk6TadplmbpPXqP9mnfKDtSRrX0XnT3koeam0svHuvR0eFKpVybN7t6elxr1rgaGly9vcXr9+93TZ6c1PX0JNulUq6tW8v3GiphTJA+nuyGHerwlFK+WZu9Rz2+Rmu8QQ3eq96i9Vll/Q7d4Xu11/drv2/RFp+syd6q1sGa7drud+pO36d9/gv9wm/RLT5Jk/wu3TVY83193+/W3f64HvdH9ahfr+s9pZQ/qAcHa7Zoi2/TNn9CT/he7fVVWuVTNdX71DdY06Y2/5l+5k/qSd+t3X65LvezdJYf1uGT6gfvxTEYzc0lvSNHtM+yhu3ixa7VqwuXNTW5NmwoXr9+fbI+f9mqVa4lS8r7D1PuMUH6eLIbLtZiX63VBcua1OQbtKHkfbxP7/MrdMWwNQu18IT7nK7p3qa2465/Ts+5JP+JfnLcmgf1oEvyR/TISfWjnP+GE+W9WGrYVsdlhMOHpd27pZaWwuUtLVJ3d/Ftdu0aWr9ihZTNSkeOjM88K12N9/GwDmu3dqtFha+nRS1Dvq4fzwN6QN3q1nItL7re5dqu7XpUj+rtenvRmn71q0MdekEvaJmWHXeum7VZp+t0LdCCojUv6kV9Q9/QOTpHczW3pPlXjBp8L1ZH2B46JPX3S42NhcsbG6W+vuLb9PUVrz96NNlfLarxPh7SIfWrX40qfD2NalSfjvP6c87W2TpVpyqjjD6uj+taXVuw/jk9pymaorTSukyX6Vbdqkt1aUHNw3pYUzRFp+pUXatrdafu1EW6qKDmR/qRpmiK6lWvm3WzOtU5ZL4btVFTcj8/1o+1Xdt1qk4daTvKqwbfi9URtgPMCn93H7rsRPXFlteaGu+jqXDeLh+y7Fg7tVNZZdWmNt2iW7RFWwrWT9VU7dEe3a/79UV9UZ/UJ7Vd2wtqLtAF2qM9uk/36WP6mK7W1dqrvQU179A7tEd71K1uXaJL9AF9QE/pqYKaq3SVHtAD6lKXztf5er/er5f00kjbUBlq6L2YKvcESjJzpjRp0tBPvIMHh37SDZg9u3h9KiXNmDE+86x0Nd7HmZqpSZo05Cz2oA4OOXs81jzNkyRdpIv0W/1WN+pGrdTKwfWn6BQ1qUmStEALtE/79CV9SRfr4sGatNKDNRlldL/u1826WV/X1wdrGtSgptzPEi3ReTpPt+k23aAbBmvOyP2cp/O0REs0XdP1PX2vYD4Vrwbfi9VxZptOJ497dHYWLu/slJYVv+alpUulbduG1mcyUl3d+Myz0tV4H9NKq1nN6lTh6+9U53GvnRbzil7Ry3q5Imo893Oi/VScWnwvjuSmW9kf/aqrc7W3J499rF2bPCZy4ECyfuXKZAzUDzwmsm5dUt/enmxf7sdEyj0mSB9PdsMOdXid6rxd7d6jHl+rtd6gBj+gA+5yX6mVvlIrB+tv1a3+Q/3QH9Nj/pge89t0m0/VVL9O1w3WfEFf8E51+hN6wnvU4zfpJk8p5Zu0abDmOl03+MjWQ3rIN2iDm8zv1t3uSp48+Kw+6/fpPu9Vr2eV9Wt0jaeVHnw87HE97l/RVzyrrPeq1+/VvX65LvdpmuZP6amT6gfvxTEYE+7RL3dXa6trzhxXOu1atMjV1fXquuXLk5Ffv2OHa+HCpH7uXNemTeWdf6WMCdDH0WzcqlafozmeVtoXaZF3qWtw3XIt9+VaPvj7zbrZ36g3+mRN9tN1ui/UQm9Vq/erf7BmgzZ4k5q8XvU+XdN9qZb6d/SdgmNerav9HJ3jaaV9lmb5xbq44JGuF/Wiv1fv9TN1pqeV9jN1pr9b7/b7dN9gzS/1S79El/gszfI61fnZOtuv1JW+T/tOuhfl/necCO/FUsPW3L3ks2DLZFzZ7DieZwOl8cq/H1IVrPT//HE8mYw8mz3hO7I6rtkCQJUjbAEgAGELAAEIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAIQtgAQgLAFgACELQAEIGwBIABhCwABCFsACEDYAkAAwhYAAhC2ABCAsAWAAIQtAAQgbAEgAGELAAEIWwAIQNgCQADCFgACELYAEICwBYAAhC0ABCBsASAAYQsAAczdSy82e1pS7/hNBwCqzhx3n3WiohGFLQDg5HAZAQACELYAEICwBYAAhC0ABCBsASAAYQsAAQhbAAhA2AJAAMIWAAL8P+0gjBydq4ztAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8fb13fd710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy_tpi, V_tpi = truncated_policy_iteration(env, max_it=2)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_tpi,\"\\n\")\n",
    "\n",
    "# plot the optimal state-value function\n",
    "plot_values(V_tpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to test your function.  If the code cell returns **PASSED**, then you have implemented the function correctly!  \n",
    "\n",
    "**Note:** In order to ensure accurate results, make sure that the `truncated_policy_iteration` function satisfies the requirements outlined above (with four inputs, two outputs, and with the default values of the input arguments unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_test.run_check('truncated_policy_iteration_check', truncated_policy_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Value Iteration\n",
    "\n",
    "In this section, you will write your own implementation of value iteration.\n",
    "\n",
    "Your algorithm should accept three arguments as input:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `theta`: This is a very small positive number that is used for the stopping criterion (default value: `1e-8`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next code cell to solve the MDP and visualize the output.  The state-value function has been reshaped to match the shape of the gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_vi, V_vi = value_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_vi,\"\\n\")\n",
    "\n",
    "# plot the optimal state-value function\n",
    "plot_values(V_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to test your function.  If the code cell returns **PASSED**, then you have implemented the function correctly!  \n",
    "\n",
    "**Note:** In order to ensure accurate results, make sure that the `value_iteration` function satisfies the requirements outlined above (with three inputs, two outputs, and with the default values of the input arguments unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_test.run_check('value_iteration_check', value_iteration)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
