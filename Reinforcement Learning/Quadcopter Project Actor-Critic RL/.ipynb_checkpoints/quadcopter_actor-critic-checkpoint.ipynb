{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aclaudioquiros/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.10</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import deque\n",
    "from task import Task\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend\n",
    "import numpy as np\n",
    "import copy\n",
    "import sys\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Experience Replay Memory buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.mem = deque(maxlen=capacity)\n",
    "        \n",
    "    def add_env_reaction(self, env_reaction):\n",
    "        # St, At, Rt1, Dt, St1.\n",
    "        self.mem.append(env_reaction)\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        indexes = np.random.choice(a=np.arange(len(self.mem)), size=batch_size, replace=False)\n",
    "        states = list()\n",
    "        actions = list()\n",
    "        rewards = list()\n",
    "        dones = list()\n",
    "        next_states = list()\n",
    "        for index in indexes:\n",
    "            st, at, rt, dt, st_1 = self.mem[index]\n",
    "            states.append(st)\n",
    "            actions.append(at)\n",
    "            rewards.append(rt)\n",
    "            dones.append(dt)\n",
    "            next_states.append(st_1)      \n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(dones), np.array(next_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor:\n",
    "    Define NN for policy approximation and specify loss, backprop with action gradients dL/dA from Critc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_space, action_space, action_range, action_min, hidden_units, name):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.action_range = action_range\n",
    "        self.action_min = action_min\n",
    "        self.name = name\n",
    "        \n",
    "        # Neural Network definition.\n",
    "        \n",
    "        # Network Architecture.\n",
    "        input_states = layers.Input(shape=(self.state_space,), dtype=np.float32, name='input_states')\n",
    "        fc1 = layers.Dense(units=hidden_units, activation='relu', name='fc1')(input_states)\n",
    "        fc2 = layers.Dense(units=2*hidden_units, activation='relu', name='fc2')(fc1)\n",
    "        fc3 = layers.Dense(units=hidden_units, activation='relu', name='fc3')(fc2)\n",
    "        norm_action = layers.Dense(self.action_space, activation='sigmoid', name='norm_action')(fc3)\n",
    "        \n",
    "        # Adapt actions for the range in which rotors work.\n",
    "        actions = layers.Lambda(lambda x: x*self.action_range + action_min, name='actions')(norm_action)\n",
    "        self.actor_model = models.Model(input=[input_states], output=[actions])\n",
    "        \n",
    "        # Define Loss\n",
    "        input_act_grad = layers.Input(shape=(self.action_space,), dtype=np.float32, name='input_act_grad')\n",
    "        loss = backend.mean(-input_act_grad*actions)\n",
    "        \n",
    "        # Get trainable parameters and define backprop optimization.\n",
    "        adam_optimizer = optimizers.Adam()\n",
    "        train_param = adam_optimizer.get_updates(params=self.actor_model.trainable_weights, loss=loss)\n",
    "        # keras.backend.learning_phase() gives a flag to be passed as input\n",
    "        # to any Keras function that uses a different behavior at train time and test time.\n",
    "        self.train_nn = backend.function(inputs=[input_states, input_act_grad, backend.learning_phase()],\\\n",
    "                                         outputs=[], updates=train_param)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic:\n",
    "    Define NN for Action value approximation and specify action gradients dL/dA to pass to Actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_space, action_space, hidden_units):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # Neural Network definition.\n",
    "        \n",
    "        # Network Architecture.\n",
    "        input_states = layers.Input(shape=(self.state_space,), dtype=np.float32, name='input_states')\n",
    "        fc_states1 = layers.Dense(units=hidden_units, activation='relu')(input_states)\n",
    "        fc_states2 = layers.Dense(units=2*hidden_units, activation='relu')(fc_states1)\n",
    "        \n",
    "        input_actions = layers.Input(shape=(self.action_space,), dtype=np.float32, name='input_actions')\n",
    "        fc_actions1 = layers.Dense(units=hidden_units, activation='relu')(input_actions)\n",
    "        fc_actions2 = layers.Dense(units=2*hidden_units, activation='relu')(fc_actions1)\n",
    "        \n",
    "        # Advantage function.\n",
    "        fc_sa1 = layers.Add()([fc_states2, fc_actions2])\n",
    "        fc_sa2 = layers.Activation('relu')(fc_sa1)\n",
    "        \n",
    "        q_values = layers.Dense(units=1, activation='relu', name='q_values')(fc_sa2)\n",
    "        self.critic_model = models.Model(inputs=[input_states, input_actions], outputs=[q_values])\n",
    "        \n",
    "        # Optimizer and Loss.\n",
    "        adam_optimizer = optimizers.Adam()\n",
    "        self.critic_model.compile(loss='mean_squared_error', optimizer=adam_optimizer)\n",
    "        \n",
    "        # Define function to get action gradients.\n",
    "        action_gradients = backend.gradients(loss=q_values, variables=[input_actions])\n",
    "        self.get_action_gradients = backend.function(inputs=[input_states, input_actions, backend.learning_phase()], \\\n",
    "                                                    outputs=action_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ornsteinâ€“Uhlenbeck process definition for exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, action_space, mean, sigma, theta):\n",
    "        self.mean = mean*np.ones(action_space)\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.restart()\n",
    "        \n",
    "    def restart(self):\n",
    "        self.current = copy.copy(self.mean)\n",
    "        \n",
    "    def sample(self):\n",
    "        x = self.current\n",
    "        dx = self.theta*(self.mean-x) + self.sigma*np.random.randn(len(x))\n",
    "        self.current = x+dx\n",
    "        return x+dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Deterministic Policy Gradient, DDPG Agent:\n",
    "    Agent definition following DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "pixieapp_metadata": null
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pixie_debugger\n",
    "\n",
    "class DDPG_Agent:\n",
    "    def __init__(self, task, noise, memory, rl_param, nn_hidden):\n",
    "        self.task = task\n",
    "        self.action_low = self.task.action_low\n",
    "        self.action_high = self.task.action_high\n",
    "        self.state_space = self.task.state_size\n",
    "        self.action_space = self.task.action_size\n",
    "        \n",
    "        # Instantiate Actors and Critics.\n",
    "        self.actor = Actor(self.state_space, self.action_space, self.action_high-self.action_low, self.action_low,\\\n",
    "                          hidden_units=nn_hidden[0], name='actor')\n",
    "        self.actor_target = Actor(self.state_space, self.action_space, self.action_high-self.action_low, \\\n",
    "                                  self.action_low, hidden_units=nn_hidden[1], name='actor_target')        \n",
    "        self.critic = Critic(self.state_space, self.action_space, hidden_units=32)\n",
    "        self.critic_target = Critic(self.state_space, self.action_space, hidden_units=32)\n",
    "        \n",
    "        # Set same weights in target.\n",
    "        self.actor_target.actor_model.set_weights(self.actor.actor_model.get_weights())\n",
    "        self.critic_target.critic_model.set_weights(self.critic.critic_model.get_weights())\n",
    "        \n",
    "        # Noise for exploration.\n",
    "        self.mean = noise[0]\n",
    "        self.sigma = noise[1]\n",
    "        self.theta = noise[2]\n",
    "        self.ounoise = OUNoise(self.action_space, self.mean, self.sigma, self.theta)\n",
    "        \n",
    "        # Experience Replay memory.\n",
    "        self.capacity = memory[0]\n",
    "        self.batch_size = memory[1]\n",
    "        self.er_buffer = ExperienceReplayBuffer(capacity=self.capacity, batch_size=self.batch_size)\n",
    "        \n",
    "        # RL parameters.\n",
    "        self.gamma = rl_param[0]\n",
    "        self.t = rl_param[1]\n",
    "        \n",
    "        # Keeping track of learning.\n",
    "        self.learning_rewards = list()\n",
    "        self.total_reward = None\n",
    "        self.best_reward = 0\n",
    "        \n",
    "    def restart_task(self):\n",
    "        if self.total_reward is not None:\n",
    "            self.learning_rewards.append(self.total_reward)\n",
    "            if self.total_reward > self.best_reward: best_reward = self.total_reward\n",
    "        self.total_reward = 0\n",
    "        self.state = self.task.reset()\n",
    "        self.ounoise.restart()\n",
    "        return self.state\n",
    "        \n",
    "    def act(self, state):\n",
    "        action = self.actor.actor_model.predict(np.reshape(state, newshape=(-1, self.state_space)))\n",
    "        self.step_noise = self.ounoise.sample()\n",
    "        action = action + self.step_noise\n",
    "        return action[0]\n",
    "        \n",
    "    # Saves expirience into memory and updates actor-critic weights.\n",
    "    def store_learn(self, state, action, reward, done, next_state):\n",
    "        \n",
    "        # Store experience into exp replay memory.\n",
    "        self.er_buffer.add_env_reaction((state, action, reward, done, next_state))\n",
    "        \n",
    "        # Learn if agent has enough experiences.\n",
    "        if len(self.er_buffer.mem) > self.batch_size:\n",
    "            self.learn()\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        # Update to the current state of the enviroment.\n",
    "        self.state = next_state\n",
    "     \n",
    "    def soft_update(self):\n",
    "        actor_current = np.array(self.actor.actor_model.get_weights())\n",
    "        critic_current = np.array(self.critic.critic_model.get_weights())\n",
    "        actor_target = np.array(self.actor_target.actor_model.get_weights())\n",
    "        critic_target = np.array(self.critic_target.critic_model.get_weights())\n",
    "        \n",
    "        self.actor_target.actor_model.set_weights(actor_target*(1-self.t) + self.t*actor_current)\n",
    "        self.critic_target.critic_model.set_weights(critic_target*(1-self.t) + self.t*critic_current)\n",
    "    \n",
    "    # Learn step of the agent, update weights of actor-critic and actor-critic target NN.\n",
    "    def learn(self):\n",
    "        states, actions, rewards, dones, next_states = self.er_buffer.sample_batch()\n",
    "        \n",
    "        # Get action for deterministic policy.\n",
    "        next_actions = self.actor_target.actor_model.predict_on_batch(next_states)\n",
    "        next_q_values = self.critic_target.critic_model.predict_on_batch([next_states, next_actions])\n",
    "        next_q_values = next_q_values.reshape((self.batch_size,))\n",
    "        \n",
    "        # Need to handle the done case.\n",
    "        targets = rewards + self.gamma*next_q_values*(1-dones)\n",
    "        self.critic.critic_model.train_on_batch(x=[states, actions],y=[targets])\n",
    "        \n",
    "        \n",
    "        # Learning Phase = 0 (Test), we just want the gradient, no update on weights.\n",
    "        action_gradients = self.critic.get_action_gradients([states, actions, 0])\n",
    "        self.actor.train_nn([states, action_gradients[0], 1])\n",
    "        \n",
    "        \n",
    "        # Do soft update on weigths.\n",
    "        self.soft_update()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run agent on the enviroment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aclaudioquiros/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# NN sizes\n",
    "actor_hidden = 32\n",
    "critic_hidden = 32\n",
    "nn_hidden = [actor_hidden, critic_hidden]\n",
    "\n",
    "# Noise for exploration.\n",
    "mean = 0\n",
    "sigma = 0.15\n",
    "theta = 0.2\n",
    "noise = [mean, sigma, theta]\n",
    "\n",
    "# RL parameters.\n",
    "gamma = 0.99\n",
    "t = 0.01\n",
    "rl_param = [gamma, t]\n",
    "\n",
    "# Experience Replay memory.\n",
    "capacity = 100000\n",
    "batch_size = 2\n",
    "memory = [capacity, batch_size]\n",
    "\n",
    "# Task parameters and instance.\n",
    "# Modify the values below to give the quadcopter a different starting position.\n",
    "runtime = 5.                                     # time limit of the episode\n",
    "init_pose = np.array([0., 0., 10., 0., 0., 0.])  # initial pose\n",
    "init_velocities = np.array([0., 0., 0.])         # initial velocities\n",
    "init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities\n",
    "file_output = 'data.txt'                         # file name for saved results\n",
    "target_pos = np.array([0., 0., 10.])\n",
    "task = Task(init_pose, init_velocities, init_angle_velocities, runtime, target_pos)\n",
    "\n",
    "\n",
    "# Pending items.\n",
    "# 4. Need to add batch norm to function approximation NN.\n",
    "quadcopter_agent = DDPG_Agent(task, noise, memory, rl_param, nn_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "pixieapp_metadata": null
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import csv\n",
    "\n",
    "\n",
    "def track_quad(task, results):\n",
    "    labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "              'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "              'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\n",
    "    results = {x : [] for x in labels}\n",
    "    for ii in range(len(labels)):\n",
    "            results[labels[ii]].append(to_write[ii])\n",
    "    line = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)\n",
    "    return line\n",
    "\n",
    "# Run the simulation, and save the results.\n",
    "with open(file_output, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(labels)\n",
    "    while True:\n",
    "        rotor_speeds = agent.act()\n",
    "        _, _, done = task.step(rotor_speeds)\n",
    "        writer.writerow(to_write)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "for episode in range(1, num_episodes+1):\n",
    "    state = quadcopter_agent.restart_task()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = quadcopter_agent.act(state)\n",
    "        next_state, reward, done = task.step(action)\n",
    "        quadcopter_agent.store_learn(state, action, reward, done, next_state)\n",
    "    print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "                episode, quadcopter_agent.total_reward, quadcopter_agent.best_reward, \\\n",
    "                quadcopter_agent.step_noise), end=\"\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
