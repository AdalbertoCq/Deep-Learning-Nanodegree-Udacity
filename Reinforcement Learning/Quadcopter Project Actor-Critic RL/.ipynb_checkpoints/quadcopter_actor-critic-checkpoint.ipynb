{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import keras.models, keras.layers, keras.optimizers, keras.backend\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Experience Replay Memory buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.mem = deque(maxlen=capacity)\n",
    "        \n",
    "    def add_env_reaction(self, env_reaction):\n",
    "        # St, At, Rt1, St1.\n",
    "        self.mem.append(env_reaction)\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        indexes = np.random.choice(a=np.arange(len(self.meme)), size=batch_size)\n",
    "        states = list()\n",
    "        actions = list()\n",
    "        rewards = list()\n",
    "        next_states = list()\n",
    "        for index in indexes:\n",
    "            st, at, rt, st_1 = self.mem[index]\n",
    "            states.append(st)\n",
    "            actions.append(at)\n",
    "            rewards.append(rt)\n",
    "            next_states.append(st_1)      \n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor:\n",
    "    Define NN for policy approximation and specify loss, backprop with action gradients dL/dA from Critc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_space, action_space, action_range, action_min, hidden_units, name):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.action_range = action_range\n",
    "        self.action_min = action_min\n",
    "        self.name = name\n",
    "        \n",
    "        # Neural Network definition.\n",
    "        \n",
    "        # Network Architecture.\n",
    "        input_states = layers.Inputs(shape=(actions_space,), dtype=keras.float32, name='input_states')\n",
    "        fc1 = layers.Dense(units=hidden_units, activation='relu', name='fc1')\n",
    "        fc2 = layers.Dense(units=2*hidden_units, activation='relu', name='fc2')\n",
    "        fc3 = layers.Dense(units=hidden_units, activation='relu', name='fc3')\n",
    "        norm_action = layers.Dense(self.action_space, activation='sigmoid', name='norm_action')\n",
    "        \n",
    "        # Adapt actions for the range in which rotors work.\n",
    "        actions = layers.Lambda(lambda x: x*self.action_range + action_min, name='actions')(norm_action)\n",
    "        self.actor_model = models.Model(input=[input_states], output=[actions])\n",
    "        \n",
    "        # Define Loss\n",
    "        input_act_grad = layers.Inputs(shape=(actions_space,), dtype=keras.float32, name='input_act_grad')\n",
    "        loss = backend.mean(-input_act_grad*actions)\n",
    "        \n",
    "        # Get trainable parameters and define backprop optimization.\n",
    "        adam_optimizer = optimizers.Adam()\n",
    "        train_param = adam_optimizer.get_updates(param=self.actor_model.trainable_weights, loss=loss)\n",
    "        # keras.backend.learning_phase() gives a flag to be passed as input\n",
    "        # to any Keras function that uses a different behavior at train time and test time.\n",
    "        self.train_nn = backend.function(inputs=[input_states, input_act_grad, backend.learning_phase()],\\\n",
    "                                         outputs=[], updates=train_param)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic:\n",
    "    Define NN for Action value approximation and specify action gradients dL/dA to pass to Actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Critc:\n",
    "    def __init__(self, state_space, action_space, hidden_units):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # Neural Network definition.\n",
    "        \n",
    "        # Network Architecture.\n",
    "        input_states = layers.Input(shape=(state_space,), dtype=keras.float32, name='input_states')\n",
    "        fc_states1 = layers.Dense(units=hidden_units, activation='relu')(input_states)\n",
    "        fc_states2 = layers.Dense(units=2*hidden_units, actication='relu')(fc_states1)\n",
    "        \n",
    "        input_actions = layers.Input(shape=(action_shape,), dtype=keras.float32, name='input_actions')\n",
    "        fc_actions1 = layers.Dense(units=hidden_units, activation='relu')(input_actions)\n",
    "        fc_actions2 = layers.Dense(units=2*hidden_units, activation='relu')(fc_actions1)\n",
    "        \n",
    "        # Advantage function.\n",
    "        fc_sa1 = layers.Add()([fc_states2, fc_actions2])\n",
    "        fc_sa2 = layers.Activation('relu')(fc_sa1)\n",
    "        q_values = layers.Dense(units=1, activation='relu', name='q_values')(fc_sa2)\n",
    "        self.critic_model = models.Model(inputs=[input_states, input_actions], outputs=[q_values])\n",
    "        \n",
    "        # Optimizer and Loss.\n",
    "        adam_optimizer = optimiziers.Adam()\n",
    "        self.critic_model.compile(loss='mean_squared_error', optimizer=adam_optimizer)\n",
    "        \n",
    "        # Define function to get action gradients.\n",
    "        action_gradients = backend.gradients(loss=loss, variables=[input_actions])\n",
    "        self.get_action_gradients = backend.function(inputs=[input_states, input_actions, backend.learning_phase()], \\\n",
    "                                                    outputs=[action_gradients])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Deterministic Policy Gradient, DDPG Agent:\n",
    "    Agent definition following DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DDPG_Agent:\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "        self.action_low = self.task.action_low\n",
    "        self.action_high = self.task.action_high\n",
    "        self.state_space = self.task.state_size\n",
    "        self.action_space = self.task.action_size\n",
    "        \n",
    "        # Instantiate Actors and Critics.\n",
    "        self.actor = Actor(self.state_space, self.action_space, self.action_high-self.action_low, self.action_low,\\\n",
    "                          hidden_units=32, name='actor')\n",
    "        self.actor_target = Actor(self.state_space, self.action_space, self.action_high-self.action_low, \\\n",
    "                                  self.action_low, hidden_units=32, name='actor_target')        \n",
    "        self.critic = Critic(self.state_space, self.action_space, hidden_units=32)\n",
    "        self.critic_target = Critic(self.state_space, self.action_space, hidden_units=32)\n",
    "        \n",
    "        # Set same weights in target.\n",
    "        self.actor_target.set_weights(self.actor.get_weights())\n",
    "        self.critic_target.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        # Noise for exploration.\n",
    "        \n",
    "        # Experience Replay memory.\n",
    "        self.capacity = 100000\n",
    "        self.batch_size = 64\n",
    "        self.er_buffer = ExperienceReplayBuffer(capacity=self.capacity, batch_size=self.batch_size)\n",
    "        \n",
    "        # RL parameters.\n",
    "        self.gamma = 0.99\n",
    "        self.t = 0.01\n",
    "        \n",
    "    # NEED TO DEBUG THIS.\n",
    "    def act(self, state):\n",
    "        action = self.actor.actor_model.predict(np.reshape(state, newshape=(-1, self.state_space)))\n",
    "        action = action + noise\n",
    "        return action\n",
    "        \n",
    "    \n",
    "    # Saves expirience into memory and updates actor-critic weights.\n",
    "    def store_learn(self, state, action, reward, next_state):\n",
    "        \n",
    "        # Store experience into exp replay memory.\n",
    "        self.er_buffer.add_env_reaction((state, action, reward, next_state))\n",
    "        \n",
    "        # Learn if agent has enough experiences.\n",
    "        if len(self.er_buffer.mem) > self.batch_size:\n",
    "            self.learn()\n",
    "        \n",
    "        # Update to the current state of the enviroment.\n",
    "        self.state = next_state\n",
    "     \n",
    "    # Learn step of the agent, update weights of actor-critic and actor-critic target NN.\n",
    "    def learn(self):\n",
    "        states, actions, rewards, next_states = self.er_buffer.sample_batch()\n",
    "        \n",
    "        # Estimated Target Q action values.\n",
    "        self.critic.critic_model.predit(states, actions)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run agent on the enviroment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
